{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":170620,"sourceType":"datasetVersion","datasetId":75548}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#install necessaries\n!pip install pytorchvideo\n!pip install pytorch_lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-31T15:27:14.972744Z","iopub.execute_input":"2023-12-31T15:27:14.973031Z","iopub.status.idle":"2023-12-31T15:27:50.832025Z","shell.execute_reply.started":"2023-12-31T15:27:14.972990Z","shell.execute_reply":"2023-12-31T15:27:50.831046Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore (from pytorchvideo)\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting av (from pytorchvideo)\n  Obtaining dependency information for av from https://files.pythonhosted.org/packages/0a/32/56aaa677f0ec369eb68623a2ade90358c1a51b6e3cd6087111bd1e096b82/av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\nCollecting parameterized (from pytorchvideo)\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\nCollecting iopath (from pytorchvideo)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pytorchvideo) (3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (1.24.3)\nCollecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (2.3.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (10.1.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore->pytorchvideo) (0.9.0)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from iopath->pytorchvideo) (4.5.0)\nCollecting portalocker (from iopath->pytorchvideo)\n  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nDownloading av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=5ea1ca71e654b9a70aa87293ecf4ad9987c8192d572a042ec5b97471cd4b2432\n  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=74332112d82643f57a31eb3b043261c458c91c1ecb0cef2a24e871cf1668faf5\n  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=7438c68f16ea005f9a91857da4431210aa1ecceb90c0bb0a7b29a5134675acd4\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\nSuccessfully built pytorchvideo fvcore iopath\nInstalling collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-11.0.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.8.2 pytorchvideo-0.1.5 yacs-0.1.8\nRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.24.3)\nRequirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2.0.0)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2023.12.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.2.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.5.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (68.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch_lightning) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:05.379796Z","iopub.execute_input":"2023-12-31T02:59:05.380195Z","iopub.status.idle":"2023-12-31T02:59:05.385399Z","shell.execute_reply.started":"2023-12-31T02:59:05.380163Z","shell.execute_reply":"2023-12-31T02:59:05.384373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Process data","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:07.585006Z","iopub.execute_input":"2023-12-31T02:59:07.585925Z","iopub.status.idle":"2023-12-31T02:59:07.819548Z","shell.execute_reply.started":"2023-12-31T02:59:07.585882Z","shell.execute_reply":"2023-12-31T02:59:07.818714Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"temporal = '/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Temporal_Anomaly_Annotation_for_Testing_Videos.txt'\ndata = pd.read_csv(temporal,sep='  ',header=None, names=['name','events','s1','e1','s2','e2'])\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformFrame(frame, size):\n    frame = cv2.resize(frame, size, interpolation=cv2.INTER_AREA)\n    return frame\ndef hflip(frame):\n    return cv2.flip(frame, 1)\ndef noisy(img, noise_type=\"gauss\"):\n    '''\n    ### Adding Noise ###\n    img: image\n    cj_type: {gauss: gaussian, sp: salt & pepper}\n    '''\n    if noise_type == \"gauss\":\n        image=img.copy() \n        mean=0\n        st=0.5\n        gauss = np.random.normal(mean,st,image.shape)\n        gauss = gauss.astype('uint8')\n        image = cv2.add(image,gauss)\n        return image\n    \n    elif noise_type == \"sp\":\n        image=img.copy() \n        prob = 0.005\n        if len(image.shape) == 2:\n            black = 0\n            white = 255            \n        else:\n            colorspace = image.shape[2]\n            if colorspace == 3:  # RGB\n                black = np.array([0, 0, 0], dtype='uint8')\n                white = np.array([255, 255, 255], dtype='uint8')\n            else:  # RGBA\n                black = np.array([0, 0, 0, 255], dtype='uint8')\n                white = np.array([255, 255, 255, 255], dtype='uint8')\n        probs = np.random.random(image.shape[:2])\n        image[probs < (prob / 2)] = black\n        image[probs > 1 - (prob / 2)] = white\n        return image\n    \n    else:\n        return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def saveVideo(path, out, start, end, num, size, **kwargs):\n    vid = cv2.VideoCapture(path)\n    length = vid.get(cv2.CAP_PROP_FRAME_COUNT)\n    first = start\n    if(end>0 and end<=length):\n        step = ((end-start)/num)\n    else:\n        first=1\n        step = min(length/2, 1024)/num\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    res = cv2.VideoWriter(out, fourcc, 10, size)\n    for i in range(num):\n        vid.set(cv2.CAP_PROP_POS_FRAMES, int(first + i*step)) # xem lai start tu 0 hay 1\n        ret, frame = vid.read()\n        frame = transformFrame(frame, size)\n        if(kwargs.get('flip', False)):\n            frame = hflip(frame)\n        frame = noisy(frame, kwargs.get('noise', 'none'))\n        res.write(frame)\n    res.release()\n    vid.release()\n    \npaths = {\n    'Abuse':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Abuse/',\n    'Arrest':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest/',\n    'Arson':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arson/',\n    'Assault':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Assault/',\n    'Burglary':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary/',\n    'Explosion':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Explosion/',\n    'Fighting':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Fighting/',\n    'Normal':'/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1/'\n} ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path = '/kaggle/working/test.mp4'\n# saveVideo('/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Abuse/Abuse001_x264.mp4',path,1,600,32,(32,32))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom glob import glob\n\n# normal = glob('/kaggle/working/abnormal/*')\n# abnormal = glob('/kaggle/working/normal/*')\n# for f in normal:\n#     os.remove(f)\n# for f in abnormal:\n#     os.remove(f)\n# os.makedirs('/kaggle/working/abnormal')\n# os.makedirs('/kaggle/working/normal')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:14.538254Z","iopub.execute_input":"2023-12-31T02:59:14.538742Z","iopub.status.idle":"2023-12-31T02:59:14.543787Z","shell.execute_reply.started":"2023-12-31T02:59:14.538704Z","shell.execute_reply":"2023-12-31T02:59:14.542611Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"size = (128, 128)\nnum = 128\nab = 0\nnm = 0\n\nfor i in range(len(data)):\n    name = '/kaggle/working/'\n    flip = [True, False]\n    noise = ['none', 'gauss']\n    if data['events'][i] not in paths.keys():\n        continue\n    if i < 188:\n        continue\n    path = paths[data['events'][i]] + data['name'][i]\n    if(data['events'][i] != 'Normal'):\n        name += 'abnormal/'\n        s1 = data['s1'][i]\n        e1 = data['e1'][i]\n        s2 = data['s2'][i]\n        e2 = data['e2'][i]\n        if(s1>0 and e1>0):\n            for j in flip:\n                for k in noise:\n                    ab += 1\n                    idx = 'ab_'+\"{:03d}\".format(ab)+'.mp4'\n                    saveVideo(path, name+idx, s1, e1, num, size, flip=j, noise=k)\n        if(s2>0 and e2>0):\n            for j in flip:\n                for k in noise:\n                    ab += 1\n                    idx = 'ab_'+\"{:03d}\".format(ab)+'.mp4'\n                    saveVideo(path, name+idx, s1, e1, num, size, flip=j, noise=k)\n    else:\n        name += 'normal/'\n        for k in noise:\n            nm += 1\n            idx = 'nm_'+\"{:03d}\".format(nm)+'.mp4'\n            saveVideo(path, name+idx, 1, -1, num, size, flip=False, noise=k)\n    print(str(i) + ' ' + data['name'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloader","metadata":{}},{"cell_type":"code","source":"normal = glob('/kaggle/working/normal/*')\nabnormal = glob('/kaggle/working/abnormal/*')\nlabel = [0]*len(normal)+[1]*len(abnormal)\ndf = pd.DataFrame(zip(normal+abnormal, label), columns=['file', 'label'])\nprint('Normal: ', len(normal))\nprint('Abnormal: ', len(abnormal))\ndf['label'] = df['label'].astype(int)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:20.233220Z","iopub.execute_input":"2023-12-31T02:59:20.233616Z","iopub.status.idle":"2023-12-31T02:59:20.264552Z","shell.execute_reply.started":"2023-12-31T02:59:20.233586Z","shell.execute_reply":"2023-12-31T02:59:20.263277Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Normal:  300\nAbnormal:  252\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                file  label\n0  /kaggle/working/normal/nm_071.mp4      0\n1  /kaggle/working/normal/nm_037.mp4      0\n2  /kaggle/working/normal/nm_078.mp4      0\n3  /kaggle/working/normal/nm_158.mp4      0\n4  /kaggle/working/normal/nm_252.mp4      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/normal/nm_071.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/normal/nm_037.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/normal/nm_078.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/normal/nm_158.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/normal/nm_252.mp4</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, shuffle=True)\nlen(train_df), len(val_df)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:24.399778Z","iopub.execute_input":"2023-12-31T02:59:24.400497Z","iopub.status.idle":"2023-12-31T02:59:24.895670Z","shell.execute_reply.started":"2023-12-31T02:59:24.400466Z","shell.execute_reply":"2023-12-31T02:59:24.894516Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                    file  label\n508  /kaggle/working/abnormal/ab_135.mp4      1\n3      /kaggle/working/normal/nm_158.mp4      0\n172    /kaggle/working/normal/nm_148.mp4      0\n101    /kaggle/working/normal/nm_056.mp4      0\n144    /kaggle/working/normal/nm_007.mp4      0\n..                                   ...    ...\n133    /kaggle/working/normal/nm_296.mp4      0\n428  /kaggle/working/abnormal/ab_028.mp4      1\n138    /kaggle/working/normal/nm_061.mp4      0\n415  /kaggle/working/abnormal/ab_221.mp4      1\n349  /kaggle/working/abnormal/ab_026.mp4      1\n\n[441 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>508</th>\n      <td>/kaggle/working/abnormal/ab_135.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/normal/nm_158.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>/kaggle/working/normal/nm_148.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>/kaggle/working/normal/nm_056.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>/kaggle/working/normal/nm_007.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>/kaggle/working/normal/nm_296.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>428</th>\n      <td>/kaggle/working/abnormal/ab_028.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>/kaggle/working/normal/nm_061.mp4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>415</th>\n      <td>/kaggle/working/abnormal/ab_221.mp4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>349</th>\n      <td>/kaggle/working/abnormal/ab_026.mp4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>441 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.to_csv('train.txt', sep=' ', index=False, header=False)\nval_df.to_csv('val.txt', sep=' ', index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:28.597900Z","iopub.execute_input":"2023-12-31T02:59:28.598615Z","iopub.status.idle":"2023-12-31T02:59:28.612194Z","shell.execute_reply.started":"2023-12-31T02:59:28.598584Z","shell.execute_reply":"2023-12-31T02:59:28.611517Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_lst = list(train_df.itertuples(index=False, name=None))\nval_lst = list(val_df.itertuples(index=False, name=None))\nprint(train_lst[0])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:31.344113Z","iopub.execute_input":"2023-12-31T02:59:31.344559Z","iopub.status.idle":"2023-12-31T02:59:31.351387Z","shell.execute_reply.started":"2023-12-31T02:59:31.344527Z","shell.execute_reply":"2023-12-31T02:59:31.350126Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"('/kaggle/working/abnormal/ab_135.mp4', 1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from pytorchvideo.data import LabeledVideoDataset, make_clip_sampler, labeled_video_dataset\nfrom pytorch_lightning import LightningDataModule\n\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    Normalize,\n    RandomShortSideScale,\n    UniformTemporalSubsample,\n    Permute\n)\n\nfrom torchvision.transforms import (\n    Compose,\n    Lambda,\n    RandomCrop,\n    RandomHorizontalFlip,\n    Resize\n)\n\nfrom torchvision.transforms._transforms_video import (\n    CenterCropVideo,\n    NormalizeVideo\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T02:59:34.099540Z","iopub.execute_input":"2023-12-31T02:59:34.099924Z","iopub.status.idle":"2023-12-31T02:59:37.545028Z","shell.execute_reply.started":"2023-12-31T02:59:34.099890Z","shell.execute_reply":"2023-12-31T02:59:37.544068Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"video_transform=Compose([\n    ApplyTransformToKey(\n        key='video',\n        transform = Compose([\n        UniformTemporalSubsample(64),\n        Lambda(lambda x: x/255),\n        Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n        RandomHorizontalFlip(p=0.5)\n        ])\n    ),\n    ApplyTransformToKey(\n        key='label',\n        transform = Lambda(lambda x: torch.tensor([float(x)]))\n    )\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:06:52.494704Z","iopub.execute_input":"2023-12-31T03:06:52.495063Z","iopub.status.idle":"2023-12-31T03:06:52.501464Z","shell.execute_reply.started":"2023-12-31T03:06:52.495034Z","shell.execute_reply":"2023-12-31T03:06:52.500494Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.BATCH_SIZE = 4\n        self.NUM_WORKERS = 2\n    \n    def train_dataloader(self):\n        train_dataset = labeled_video_dataset(\n            '/kaggle/working/train.txt',\n            clip_sampler=make_clip_sampler('random', 12.8),\n            transform=video_transform,\n            decode_audio=False\n        )\n        train_loader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, num_workers=self.NUM_WORKERS, shuffle=False)\n        return train_loader\n    \n    def val_dataloader(self):\n        val_dataset = labeled_video_dataset(\n            '/kaggle/working/val.txt',\n            clip_sampler=make_clip_sampler('random', 12.8),\n            transform=video_transform,\n            decode_audio=False\n        )\n        val_loader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, num_workers=self.NUM_WORKERS, shuffle=False)\n        return val_loader\n    \n    def test_dataloader(self):\n        val_dataset = labeled_video_dataset(\n            '/kaggle/working/val.txt',\n            clip_sampler=make_clip_sampler('random', 6.4),\n            transform=video_transform,\n            decode_audio=False\n        )\n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n        return val_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:06:54.444063Z","iopub.execute_input":"2023-12-31T03:06:54.444828Z","iopub.status.idle":"2023-12-31T03:06:54.454110Z","shell.execute_reply.started":"2023-12-31T03:06:54.444786Z","shell.execute_reply":"2023-12-31T03:06:54.453074Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"dm = MyDataModule()\nbatch=next(iter(dm.val_dataloader()))\nbatch.keys()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:06:56.414503Z","iopub.execute_input":"2023-12-31T03:06:56.415419Z","iopub.status.idle":"2023-12-31T03:06:57.419571Z","shell.execute_reply.started":"2023-12-31T03:06:56.415386Z","shell.execute_reply":"2023-12-31T03:06:57.418451Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"dict_keys(['video', 'video_name', 'video_index', 'clip_index', 'aug_index', 'label'])"},"metadata":{}}]},{"cell_type":"code","source":"batch['video'].shape, batch['label'].shape, batch['video_name'], batch['label']","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:07:27.389858Z","iopub.execute_input":"2023-12-31T03:07:27.390249Z","iopub.status.idle":"2023-12-31T03:07:27.398113Z","shell.execute_reply.started":"2023-12-31T03:07:27.390193Z","shell.execute_reply":"2023-12-31T03:07:27.397171Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(torch.Size([2, 3, 64, 128, 128]),\n torch.Size([2, 1]),\n ['nm_133.mp4', 'nm_058.mp4'],\n tensor([[0.],\n         [0.]]))"},"metadata":{}}]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pytorch_lightning import LightningModule, seed_everything, Trainer\nfrom pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import classification_report\nimport torchmetrics","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:07:44.111755Z","iopub.execute_input":"2023-12-31T03:07:44.112516Z","iopub.status.idle":"2023-12-31T03:07:44.117482Z","shell.execute_reply.started":"2023-12-31T03:07:44.112482Z","shell.execute_reply":"2023-12-31T03:07:44.116525Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"video_model = torch.hub.load('facebookresearch/pytorchvideo', model='efficient_x3d_xs', pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:07:46.609185Z","iopub.execute_input":"2023-12-31T03:07:46.610010Z","iopub.status.idle":"2023-12-31T03:07:47.240813Z","shell.execute_reply.started":"2023-12-31T03:07:46.609979Z","shell.execute_reply":"2023-12-31T03:07:47.239821Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        vid_mod = torch.hub.load('facebookresearch/pytorchvideo', model='efficient_x3d_xs', pretrained=True)\n        self.vid_model = vid_mod\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(400, 1)\n        #parameters\n        self.lr=1e-3\n        self.batch_size = 4\n        #loss function\n        self.loss = nn.BCEWithLogitsLoss()\n    \n    def forward(self, x):\n        x = self.vid_model(x)\n        x = self.relu(x)\n        x = self.linear(x)\n        return x\n        \n    def training_step(self, batch, batch_idx):\n        y_hat = self(batch['video'])\n        loss = self.loss(y_hat, batch['label'])\n        self.log(\"train_loss\", loss, sync_dist=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        y_hat = self(batch['video'])\n        loss = self.loss(y_hat, batch['label'])\n        self.log(\"val_loss\", loss, sync_dist=True)\n        return loss\n    \n    def configure_optimizers(self):\n        opt = torch.optim.AdamW(params=self.parameters(), lr=self.lr)\n        return {'optimizer':opt}","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:09:21.733103Z","iopub.execute_input":"2023-12-31T03:09:21.733511Z","iopub.status.idle":"2023-12-31T03:09:21.743516Z","shell.execute_reply.started":"2023-12-31T03:09:21.733479Z","shell.execute_reply":"2023-12-31T03:09:21.742546Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"m = MyModel()\n#print(batch['video'])\nout = m(batch['video'])\nprint(out.shape, out)\nlab = batch['label']\nprint(lab.shape, lab)\nl = m.loss\nprint(l(out, lab))","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:09:23.769156Z","iopub.execute_input":"2023-12-31T03:09:23.770102Z","iopub.status.idle":"2023-12-31T03:09:29.135374Z","shell.execute_reply.started":"2023-12-31T03:09:23.770057Z","shell.execute_reply":"2023-12-31T03:09:29.134234Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n","output_type":"stream"},{"name":"stdout","text":"torch.Size([2, 1]) tensor([[-0.5269],\n        [ 0.1899]], grad_fn=<AddmmBackward0>)\ntorch.Size([2, 1]) tensor([[0.],\n        [0.]])\ntensor(0.6283, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning.loggers import WandbLogger\nimport wandb\nwandb.login(key=\"1658d378d091cf8659e37004bc727f76b3de8356\")\nwandb.init(project=\"banhgao1\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:19:34.242905Z","iopub.execute_input":"2023-12-31T03:19:34.243285Z","iopub.status.idle":"2023-12-31T03:20:07.895242Z","shell.execute_reply.started":"2023-12-31T03:19:34.243256Z","shell.execute_reply":"2023-12-31T03:20:07.894250Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mduongchinh29062003\u001b[0m (\u001b[33mchinh\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231231_031936-gzkzuogm</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/chinh/banhgao1/runs/gzkzuogm' target=\"_blank\">soft-capybara-2</a></strong> to <a href='https://wandb.ai/chinh/banhgao1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/chinh/banhgao1' target=\"_blank\">https://wandb.ai/chinh/banhgao1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/chinh/banhgao1/runs/gzkzuogm' target=\"_blank\">https://wandb.ai/chinh/banhgao1/runs/gzkzuogm</a>"},"metadata":{}},{"execution_count":49,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/chinh/banhgao1/runs/gzkzuogm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79e3f6118df0>"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(monitor='val_loss',mode='min',save_top_k=10,dirpath='/kaggle/working/log',filename='banhgao-{val_loss:.2f}')\nlr_monitor = LearningRateMonitor(logging_interval='epoch')\n#os.makedirs(\"/kaggle/working/log\")","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:53:29.130318Z","iopub.execute_input":"2023-12-31T03:53:29.130696Z","iopub.status.idle":"2023-12-31T03:53:29.136240Z","shell.execute_reply.started":"2023-12-31T03:53:29.130664Z","shell.execute_reply":"2023-12-31T03:53:29.135347Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model = MyModel()\ndata_module = MyDataModule()\nwandb_logger = WandbLogger(log_model='all',save_dir=\"/kaggle/working/log\",project='banhgao1')\ntrainer = Trainer(devices=2, accelerator='gpu', max_epochs=32, logger = wandb_logger, default_root_dir=\"/kaggle/working/log\")\ntrainer.fit(model,data_module)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T03:53:32.234869Z","iopub.execute_input":"2023-12-31T03:53:32.235300Z","iopub.status.idle":"2023-12-31T04:29:29.606398Z","shell.execute_reply.started":"2023-12-31T03:53:32.235267Z","shell.execute_reply":"2023-12-31T04:29:29.605088Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ec4da309914323b2eb40a99d56be00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T01:05:19.173204Z","iopub.execute_input":"2023-12-30T01:05:19.174058Z","iopub.status.idle":"2023-12-30T01:05:19.178684Z","shell.execute_reply.started":"2023-12-30T01:05:19.174019Z","shell.execute_reply":"2023-12-30T01:05:19.177566Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}